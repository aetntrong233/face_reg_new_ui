{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import prepare_dataset\n",
    "\n",
    "INPUT_DIR = ''\n",
    "ALIGNED_DIR = 'dataset/aligned'\n",
    "MASKED_DIR = 'dataset/masked'\n",
    "COMBINE_DIR = 'dataset/combine'\n",
    "\n",
    "print('INFO: PREPARE DATASET')\n",
    "\n",
    "print('INFO: Align face')\n",
    "print('Please wait...')\n",
    "prepare_dataset.aligned_dataset(INPUT_DIR, ALIGNED_DIR)\n",
    "print('Aligned face save at ' + ALIGNED_DIR)\n",
    "print('INFO: Remove face mask region')\n",
    "print('Please wait...')\n",
    "prepare_dataset.masked_dataset(ALIGNED_DIR, MASKED_DIR)\n",
    "print('Masked face save at ' + MASKED_DIR)\n",
    "print('INFO: Combine dataset')\n",
    "print('Please wait...')\n",
    "prepare_dataset.combine_dataset(ALIGNED_DIR, MASKED_DIR, COMBINE_DIR)\n",
    "print('Combine dataset save at ' + COMBINE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "in_dir = r'D:\\sw\\face_rec\\data\\fei\\fei_aligned'\n",
    "DATA_DIR = r'D:\\sw\\face_rec\\data\\fei\\fei_crop'\n",
    "for fld_name in os.listdir(in_dir):\n",
    "    fld_path = os.path.join(in_dir, fld_name)\n",
    "    new_fld_path = os.path.join(DATA_DIR, fld_name)\n",
    "    os.mkdir(new_fld_path)\n",
    "    for file_name in os.listdir(fld_path):\n",
    "        file_path = os.path.join(fld_path, file_name)\n",
    "        new_file_path = os.path.join(new_fld_path, file_name)\n",
    "        img = cv2.imread(file_path)\n",
    "        center = (img.shape[1]//2, img.shape[0]//2)\n",
    "        pad_x = int(12.5*img.shape[1]/100)\n",
    "        pad_y = int(12.5*img.shape[0]/100)\n",
    "        ret_img = img.copy()[pad_y:img.shape[1]-pad_y, pad_x:img.shape[0]-pad_x]\n",
    "        cv2.imwrite(new_file_path, ret_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.metrics import CategoricalAccuracy\n",
    "import os\n",
    "from models.feature_extraction_model.inceptionresnetv2 import get_train_model\n",
    "\n",
    "\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 128\n",
    "# DATA_DIR = COMBINE_DIR\n",
    "RESULTS_DIR = 'train/results'\n",
    "\n",
    "print('INFO: TRAIN MODEL')\n",
    "\n",
    "print('INFO: Dataset pre_processing')\n",
    "print('Please wait...')\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATA_DIR,\n",
    "    label_mode = 'categorical',\n",
    "    image_size=(299,299),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    subset = \"training\",\n",
    "    validation_split = 0.2,\n",
    "    seed = 23,\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATA_DIR,\n",
    "    label_mode = 'categorical',\n",
    "    image_size=(299,299),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    subset = \"validation\",\n",
    "    validation_split = 0.2,\n",
    "    seed = 23,\n",
    ")\n",
    "\n",
    "num_class = len(os.listdir(DATA_DIR))\n",
    "\n",
    "train_ds = train_ds.prefetch(buffer_size=BATCH_SIZE)\n",
    "val_ds = val_ds.prefetch(buffer_size=BATCH_SIZE)\n",
    "\n",
    "# data_augmentation = keras.layers.RandomZoom(0.2)\n",
    "# augmented_train_ds = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
    "normalization_layer = keras.layers.Rescaling(1./255)\n",
    "normalized_train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "normalized_val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "\n",
    "print('INFO: Create model')\n",
    "model = get_train_model(num_class)\n",
    "\n",
    "print('INFO: Compile model')\n",
    "model.compile(loss='categorical_crossentropy',optimizer='Adam', metrics=[CategoricalAccuracy()])\n",
    "\n",
    "# initial_learning_rate = 0.001\n",
    "\n",
    "# def lr_exp_decay(epoch, lr):\n",
    "#     k = 0.1\n",
    "#     return initial_learning_rate * math.exp(-k*epoch)\n",
    "\n",
    "callbacks = [\n",
    "    # keras.callbacks.LearningRateScheduler(lr_exp_decay, verbose=1),\n",
    "    keras.callbacks.ModelCheckpoint(os.path.join(RESULTS_DIR, \"check_point.h5\"), monitor='categorical_accuracy', verbose=1, save_best_only=True, save_weights_only=False, mode='max'),\n",
    "]\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(\n",
    "    normalized_train_ds,\n",
    "    epochs = EPOCHS,\n",
    "    callbacks = callbacks,\n",
    "    validation_data = normalized_val_ds,\n",
    ")\n",
    "\n",
    "model.save(os.path.join(RESULTS_DIR, \"final.h5\"))\n",
    "\n",
    "epochs = [i for i in range(1, len(history.history['loss'])+1)]\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(epochs, history.history['categorical_accuracy'], color='blue', label=\"training_accuracy\")\n",
    "plt.legend(loc='best')\n",
    "plt.title('training')\n",
    "plt.xlabel('epoch')\n",
    "plt.savefig(os.path.join(RESULTS_DIR, \"acc.png\"), bbox_inches='tight')\n",
    "# plt.show()\n",
    "plt.figure(2)\n",
    "plt.plot(epochs, history.history['loss'], color='red', label=\"training_loss\")\n",
    "plt.legend(loc='best')\n",
    "plt.title('training')\n",
    "plt.xlabel('epoch')\n",
    "plt.savefig(os.path.join(RESULTS_DIR, \"loss.png\"), bbox_inches='tight')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert trained model to embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 299, 299, 3)]     0         \n",
      "                                                                 \n",
      " inception_resnet_v2 (Functi  (None, 8, 8, 1536)       54336736  \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " global_average_pooling2d_1   (None, 1536)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 1536)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2048)              3145728   \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1024)              2097152   \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 512)               524288    \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " batch_normalization_407 (Ba  (None, 512)              2048      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 60,105,952\n",
      "Trainable params: 5,768,192\n",
      "Non-trainable params: 54,337,760\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from models.feature_extraction_model.inceptionresnetv2 import get_train_model\n",
    "from keras.models import Model\n",
    "\n",
    "base_model = get_train_model(515)\n",
    "base_model.load_weights('train/results/check_point.h5')\n",
    "model = Model(base_model.inputs, base_model.layers[-2].output)\n",
    "model.save_weights('models/feature_extraction_model/inceptionresnetv2_512_weights.h5')\n",
    "# model = cvt_trained_md_2_embedding_md('train/results/97329.h5', 'models/feature_extraction_model/inceptionresnetv2_512_weights.h5')\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
